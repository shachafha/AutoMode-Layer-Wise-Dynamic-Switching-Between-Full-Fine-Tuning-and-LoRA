{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoMode GLUE Experiments\n",
        "\n",
        "This notebook implements a dynamic fine-tuning strategy for GLUE tasks. \n",
        "It includes implementations for:\n",
        "- **LoRA** (Low-Rank Adaptation)\n",
        "- **Dynamic Gradient Norm** (Adaptive Layer Freezing)\n",
        "- **BitFit** and **Top-K** baselines\n",
        "- **Full Fine-Tuning**\n",
        "\n",
        "Experiments are driven by a configuration CSV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32G_xvnoHvI8",
        "outputId": "894ecaa3-e01f-4dbd-a870-d962662e12c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "64KLhcRgaHxL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate peft pandas torch numpy -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    get_scheduler\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5sLphS4IjWG",
        "outputId": "9ace17aa-cb0b-40fd-ceb9-1c563db8a0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All results will be saved to: /home/jupyter/NLP_Project_Results_dec7/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# --- DEFINE SAVE PATH ---\n",
        "# Ensure you have created this directory in your Google Drive or local machine.\n",
        "SAVE_PATH = \"/home/jupyter/NLP_Project_Results_dec7/\"\n",
        "\n",
        "# Create directory if it does not exist\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "print(f\"All results will be saved to: {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jBvz1ZcYbkxS"
      },
      "outputs": [],
      "source": [
        "TASK_TO_KEYS = {\n",
        "    'sst2': ('sentence', None),\n",
        "    'rte': ('sentence1', 'sentence2'),\n",
        "    'qnli': ('question', 'sentence'),\n",
        "    'mrpc': ('sentence1', 'sentence2'),\n",
        "    'mnli': ('premise', 'hypothesis'),\n",
        "    'stsb': ('sentence1', 'sentence2'),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "J-qjToVshWpJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Checkpointing\n",
        "Logic to skip experiments that have already been completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNilm1czuwCI",
        "outputId": "4d60b2f8-0dcf-4eba-c24e-4aa7a8327f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 38 completed experiments. They will be skipped.\n"
          ]
        }
      ],
      "source": [
        "# --- CHECKPOINTING LOGIC ---\n",
        "intermediate_csv_path = os.path.join(SAVE_PATH, \"master_results_log_intermediate.csv\")\n",
        "completed_exp_ids = set()\n",
        "\n",
        "try:\n",
        "    df_intermediate = pd.read_csv(intermediate_csv_path)\n",
        "    # Ensure 'exp_id' is read as a zero-padded string to match '0001'\n",
        "    completed_exp_ids = set(df_intermediate['exp_id'].astype(str))\n",
        "    print(f\"Found {len(completed_exp_ids)} completed experiments. They will be skipped.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No intermediate results file found. Starting a fresh run.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Intermediate results file is empty. Starting a fresh run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Search Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHHN8qX3gHdb"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# --- ‚öôÔ∏è HYPERPARAMETER SEARCH SPACE ‚öôÔ∏è ---\n",
        "# =======================================================\n",
        "\n",
        "# --- Global Configs ---\n",
        "GLUE_TASKS = ['qnli','sst2','rte','mrpc']\n",
        "MODEL_CHECKPOINTS = ['distilbert-base-uncased','roberta-base','bert-base-uncased']\n",
        "STRATEGIES = ['dynamic_grad_norm']\n",
        "SEEDS = [42,25,8] \n",
        "LEARNING_RATES = [2e-5,3e-5]\n",
        "BATCH_SIZE = 32 \n",
        "\n",
        "\n",
        "\n",
        "# --- Task-Specific Configs ---\n",
        "EPOCHS_PER_TASK = {\n",
        "    'sst2': 3,     # Medium-Large dataset\n",
        "    'rte': 7,     # Tiny dataset\n",
        "    'qnli': 3,     # Large dataset\n",
        "    'mrpc': 7,     # Tiny dataset\n",
        "}\n",
        "\n",
        "# --- Strategy-Specific Configs ---\n",
        "\n",
        "# 1. LoRA Configs\n",
        "LORA_CONFIGS = [\n",
        "    {'r': 4, 'lora_alpha': 8, 'lora_dropout': 0.1},\n",
        "    {'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1},\n",
        "    {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1}\n",
        "]\n",
        "\n",
        "# 2. Dynamic Method Configs\n",
        "DYNAMIC_CONFIGS = [\n",
        "    {'num_updates_per_epoch': 6, 'threshold_percentile': 10},\n",
        "    {'num_updates_per_epoch': 6, 'threshold_percentile': 25},\n",
        "    {'num_updates_per_epoch': 10, 'threshold_percentile': 10},\n",
        "    {'num_updates_per_epoch': 10, 'threshold_percentile': 25},\n",
        "]\n",
        "TOPK_VALUES = [1,2,3,4,5]   # iterate over any k you want\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Loading and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2Bjk3_0bGAv"
      },
      "outputs": [],
      "source": [
        "def get_layer_name(param_name, model_type):\n",
        "    \"\"\"\n",
        "    Groups parameters by their Transformer layer.\n",
        "    Handles names from both base models and PEFT-wrapped models.\n",
        "    e.g., 'base_model.model.bert.encoder.layer.0.attention.self.query.weight' -> 'bert.encoder.layer.0'\n",
        "    e.g., 'bert.encoder.layer.0.attention.self.query.weight' -> 'bert.encoder.layer.0'\n",
        "    \"\"\"\n",
        "    parts = param_name.split('.')\n",
        "\n",
        "    # Find the model prefix (bert, roberta, distilbert)\n",
        "    model_prefix_index = -1\n",
        "    if model_type in parts:\n",
        "        model_prefix_index = parts.index(model_type)\n",
        "\n",
        "    if model_prefix_index != -1 and 'layer' in parts[model_prefix_index:]:\n",
        "        # Find 'layer' *after* the model type\n",
        "        layer_index = parts.index('layer', model_prefix_index)\n",
        "        return \".\".join(parts[model_prefix_index:layer_index+2])\n",
        "\n",
        "    return 'other_params' # Embeddings, pooler, classifier\n",
        "\n",
        "def load_data(task_name, model_checkpoint):\n",
        "    \"\"\"Loads and tokenizes a GLUE task, handling regression for STS-B.\"\"\"\n",
        "    print(f\"Loading dataset for task: {task_name}\")\n",
        "    dataset = load_dataset(\"glue\", task_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        key1, key2 = TASK_TO_KEYS[task_name]\n",
        "        args = (examples[key1],) if key2 is None else (examples[key1], examples[key2])\n",
        "        return tokenizer(*args, truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Rename 'label' to 'labels' and set format\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "\n",
        "    tokenized_datasets.set_format(\n",
        "        \"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        tokenized_datasets['train'],\n",
        "        shuffle=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    # Handle MNLI dual-validation\n",
        "    if task_name == 'mnli':\n",
        "        eval_dataloaders = {\n",
        "            'validation_matched': DataLoader(\n",
        "                tokenized_datasets['validation_matched'],\n",
        "                batch_size=BATCH_SIZE,\n",
        "                collate_fn=data_collator\n",
        "            ),\n",
        "            'validation_mismatched': DataLoader(\n",
        "                tokenized_datasets['validation_mismatched'],\n",
        "                batch_size=BATCH_SIZE,\n",
        "                collate_fn=data_collator\n",
        "            )\n",
        "        }\n",
        "    else:\n",
        "        eval_dataloaders = {\n",
        "            'validation': DataLoader(\n",
        "                tokenized_datasets['validation'],\n",
        "                batch_size=BATCH_SIZE,\n",
        "                collate_fn=data_collator\n",
        "            )\n",
        "        }\n",
        "\n",
        "    num_labels = dataset['train'].features['label'].num_classes\n",
        "\n",
        "    return train_dataloader, eval_dataloaders, num_labels\n",
        "\n",
        "def get_model(tuning_strategy, model_checkpoint, num_labels, lora_config, dynamic_config=None):\n",
        "    \"\"\"\n",
        "    Loads the model based on the hybrid strategy.\n",
        "    Now accepts a lora_config dict.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model: {model_checkpoint} with strategy: {tuning_strategy}\")\n",
        "\n",
        "    task_type = TaskType.SEQ_CLS\n",
        "\n",
        "    # --- THIS IS THE CHANGE ---\n",
        "    # Parameters are now read from the config\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=task_type,\n",
        "        inference_mode=False,\n",
        "        r=lora_config.get('r', 8), # .get() provides a default\n",
        "        lora_alpha=lora_config.get('lora_alpha', 16),\n",
        "        lora_dropout=lora_config.get('lora_dropout', 0.1),\n",
        "        target_modules=[\"query\", \"value\", \"q_lin\", \"v_lin\"]\n",
        "    )\n",
        "    # --- END OF CHANGE ---\n",
        "\n",
        "    # Base model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    if tuning_strategy == 'lora':\n",
        "        model = get_peft_model(model, peft_config)\n",
        "    elif tuning_strategy == 'full_ft':\n",
        "        pass\n",
        "    elif tuning_strategy.startswith('dynamic_'):\n",
        "        model = get_peft_model(model, peft_config)\n",
        "    elif tuning_strategy == 'bitfit':\n",
        "        # Bias-only fine-tuning\n",
        "        apply_bitfit(model)\n",
        "    elif tuning_strategy == 'topk_full':\n",
        "        model_type = model_checkpoint.split('-')[0]\n",
        "        # Top-k blocks fully fine-tuned, everything else frozen\n",
        "        apply_topk_full_ft(model, model_type=model_type, k=dynamic_config['topk_layers'])\n",
        "    else:\n",
        "        raise ValueError(\"Unknown tuning strategy\")\n",
        "\n",
        "    print(\"Trainable parameters:\")\n",
        "    if tuning_strategy in ['full_ft', 'bitfit', 'topk_full']:\n",
        "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: 100.00\")\n",
        "    else:\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_optimizer(model, learning_rate):\n",
        "    \"\"\"Creates a new optimizer for ONLY the currently trainable parameters.\"\"\"\n",
        "    return torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=learning_rate\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaQw6VUzch6n"
      },
      "outputs": [],
      "source": [
        "## 4. ‚ùÑÔ∏è Dynamic Freezing Implementation (\n",
        "\n",
        "# This dictionary will store tuples: (sum_of_squared_norms, parameter_count)\n",
        "gradient_accumulator = defaultdict(lambda: (0.0, 0))\n",
        "freezing_log = []\n",
        "\n",
        "def accumulate_gradients(model, model_type):\n",
        "    \"\"\"\n",
        "    Accumulates squared gradients AND parameter counts for each logical layer.\n",
        "    \"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None and param.requires_grad:\n",
        "            layer_name = get_layer_name(name, model_type)\n",
        "            if layer_name != 'other_params':\n",
        "                current_norm_sq, current_count = gradient_accumulator[layer_name]\n",
        "\n",
        "                # Add this parameter's contribution\n",
        "                new_norm_sq = current_norm_sq + (torch.norm(param.grad, p=2).item()**2)\n",
        "                new_count = current_count + param.numel()\n",
        "\n",
        "                gradient_accumulator[layer_name] = (new_norm_sq, new_count)\n",
        "\n",
        "# We need to import the LoRA layer types to find them\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "\n",
        "def update_frozen_layers_HYBRID(model, model_type, threshold_percentile, global_step):\n",
        "    \"\"\"\n",
        "    The core of \"Adaptive-Freeze LoRA\" - CORRECTED.\n",
        "    1. Compares AVERAGE gradient norms.\n",
        "    2. MERGES LoRA weights before switching to Full-FT.\n",
        "    3. RESETS LoRA weights before switching to LoRA-Frozen.\n",
        "    \"\"\"\n",
        "    if not gradient_accumulator:\n",
        "        return False\n",
        "\n",
        "    # 1. Calculate AVERAGE gradient norm for each layer\n",
        "    avg_layer_norms = {}\n",
        "    for layer, (sum_sq_norm, param_count) in gradient_accumulator.items():\n",
        "        if param_count > 0:\n",
        "            # We add a small epsilon to prevent division by zero, just in case\n",
        "            avg_norm = (sum_sq_norm / (param_count + 1e-9))**0.5\n",
        "            avg_layer_norms[layer] = avg_norm\n",
        "\n",
        "    if not avg_layer_norms:\n",
        "        return False\n",
        "\n",
        "    # 2. Determine the threshold based on average norms\n",
        "    norms = list(avg_layer_norms.values())\n",
        "    if not norms:\n",
        "        return False\n",
        "\n",
        "    threshold_val = np.percentile(norms, threshold_percentile)\n",
        "\n",
        "    print(f\"\\n--- Dynamic Hybrid Check @ Step {global_step+1} ---\")\n",
        "    print(f\"AVERAGE Gradient Norm {threshold_percentile}th percentile threshold: {threshold_val}\")\n",
        "\n",
        "    # 3. Build a map of what each layer's target state should be\n",
        "    target_state_map = {} # {'layer_name': 'full_ft' or 'lora_frozen'}\n",
        "    log_entry = {'step': global_step+1, 'threshold': threshold_val, 'layers': {}}\n",
        "\n",
        "    for layer_name, avg_norm in avg_layer_norms.items():\n",
        "        is_full_ft_target = (avg_norm >= threshold_val)\n",
        "        target_state = 'full_ft' if is_full_ft_target else 'lora_frozen'\n",
        "        target_state_map[layer_name] = target_state\n",
        "        log_entry['layers'][layer_name] = {'norm': avg_norm, 'action': target_state}\n",
        "\n",
        "    # 4. Apply the state changes by iterating through the model's named modules\n",
        "    params_changed = False\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # We iterate over NAMED modules to get both the name and the module\n",
        "    for module_name, module in model.named_modules():\n",
        "    # --- END OF FIX ---\n",
        "\n",
        "        if isinstance(module, LoraLayer):\n",
        "            # The 'module_name' is the full string name, like:\n",
        "            # 'base_model.model.distilbert.transformer.layer.0.attention.q_lin'\n",
        "\n",
        "            # --- THIS IS THE FIX ---\n",
        "            # We pass the string 'module_name' to get_layer_name\n",
        "            layer_name = get_layer_name(module_name, model_type)\n",
        "            # --- END OF FIX ---\n",
        "\n",
        "            if layer_name in target_state_map:\n",
        "                target_state = target_state_map[layer_name]\n",
        "\n",
        "                # Check current state by seeing if the base layer is trainable\n",
        "                is_currently_full_ft = next(module.get_base_layer().parameters()).requires_grad\n",
        "\n",
        "                if target_state == 'full_ft' and not is_currently_full_ft:\n",
        "                    # --- SWITCHING: LoRA-Frozen -> Full-FT ---\n",
        "                    print(f\"Switching layer {layer_name} to Full-FT (merging weights)...\")\n",
        "\n",
        "                    # 1. Merge LoRA weights into base weights\n",
        "                    module.merge()\n",
        "\n",
        "                    # 2. Freeze LoRA adapters\n",
        "                    for param in module.lora_A.parameters(): param.requires_grad = False\n",
        "                    for param in module.lora_B.parameters(): param.requires_grad = False\n",
        "\n",
        "                    # 3. Unfreeze base layer\n",
        "                    for param in module.get_base_layer().parameters(): param.requires_grad = True\n",
        "\n",
        "                    params_changed = True\n",
        "\n",
        "                elif target_state == 'lora_frozen' and is_currently_full_ft:\n",
        "                    # --- SWITCHING: Full-FT -> LoRA-Frozen ---\n",
        "                    print(f\"Switching layer {layer_name} to LoRA-Frozen (resetting adapters)...\")\n",
        "\n",
        "                    # 1. Freeze base layer\n",
        "                    for param in module.get_base_layer().parameters(): param.requires_grad = False\n",
        "\n",
        "                    # 2. Unfreeze LoRA adapters\n",
        "                    for param in module.lora_A.parameters(): param.requires_grad = True\n",
        "                    for param in module.lora_B.parameters(): param.requires_grad = True\n",
        "\n",
        "                    # 3. Reset LoRA weights to zero\n",
        "                    module.reset_lora_parameters(\"default\", True)\n",
        "\n",
        "                    params_changed = True\n",
        "\n",
        "    if params_changed:\n",
        "        print(\"Toggled layer states between Full-FT and LoRA-Frozen.\")\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "    # Calculate and log the *actual* trainable param count at this step\n",
        "    current_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    log_entry['current_trainable_params'] = current_trainable_params\n",
        "\n",
        "    # Clear the accumulator for the next cycle\n",
        "    gradient_accumulator.clear()\n",
        "\n",
        "    # Log the decision\n",
        "    freezing_log.append(log_entry)\n",
        "\n",
        "    return params_changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dynamic Freezing Logic\n",
        "\n",
        "This section contains the core logic for the **Dynamic Gradient Norm** strategy.\n",
        "It tracks gradient norms per layer and freezes/unfreezes layers based on a percentile threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bvGyKVnd957u"
      },
      "outputs": [],
      "source": [
        "def apply_bitfit(model):\n",
        "    \"\"\"\n",
        "    BitFit: freeze all weights, train only bias terms + classifier head.\n",
        "    \"\"\"\n",
        "    # 1. Freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 2. Enable only bias terms\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"bias\" in name:\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # 3. Make sure classifier head is fully trainable\n",
        "    if hasattr(model, \"classifier\"):\n",
        "        for p in model.classifier.parameters():\n",
        "            p.requires_grad = True\n",
        "    else:\n",
        "        # Fallback: if classifier is named differently, unfreeze any param with 'classifier' in its name\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"classifier\" in name:\n",
        "                p.requires_grad = True\n",
        "\n",
        "    print(\"Applied BitFit: only bias terms + classifier head are trainable.\")\n",
        "\n",
        "\n",
        "def apply_topk_full_ft(model, model_type, k):\n",
        "    \"\"\"\n",
        "    Top-k full fine-tuning baseline (no LoRA):\n",
        "    - Freeze all parameters.\n",
        "    - Unfreeze only the top-k Transformer blocks + classifier head.\n",
        "    \"\"\"\n",
        "    # Freeze everything first\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Get the backbone (bert / roberta / distilbert)\n",
        "    backbone = getattr(model, model_type, None)\n",
        "    if backbone is None:\n",
        "        print(f\"[WARN] Could not find backbone '{model_type}' on model. \"\n",
        "              f\"Falling back to full fine-tuning.\")\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "    else:\n",
        "        # Try to find encoder layers\n",
        "        layers = None\n",
        "        if hasattr(backbone, \"encoder\") and hasattr(backbone.encoder, \"layer\"):\n",
        "            layers = backbone.encoder.layer\n",
        "        elif hasattr(backbone, \"transformer\") and hasattr(backbone.transformer, \"layer\"):\n",
        "            layers = backbone.transformer.layer\n",
        "\n",
        "        if layers is None:\n",
        "            print(f\"[WARN] Could not find encoder.layer/transformer.layer on '{model_type}'. \"\n",
        "                  f\"Falling back to full fine-tuning.\")\n",
        "            for p in model.parameters():\n",
        "                p.requires_grad = True\n",
        "        else:\n",
        "            n = len(layers)\n",
        "            k = min(k, n)\n",
        "            top_indices = list(range(n - k, n))\n",
        "            print(f\"Applying top-k full FT: unfreezing top {k} layers: {top_indices}\")\n",
        "\n",
        "            # Unfreeze only top-k layers\n",
        "            for i in top_indices:\n",
        "                for p in layers[i].parameters():\n",
        "                    p.requires_grad = True\n",
        "\n",
        "    # Always unfreeze classifier head\n",
        "    if hasattr(model, \"classifier\"):\n",
        "        for p in model.classifier.parameters():\n",
        "            p.requires_grad = True\n",
        "    else:\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"classifier\" in name:\n",
        "                p.requires_grad = True\n",
        "\n",
        "    print(\"Applied top-k full FT baseline.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Baseline Strategies (BitFit & Top-K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lVGdkCpclox"
      },
      "outputs": [],
      "source": [
        "def custom_train(config):\n",
        "    \"\"\"\n",
        "    A custom training loop that is now fully driven by the 'config' dict.\n",
        "    \"\"\"\n",
        "    # --- 1. SET SEED ---\n",
        "    set_seed(config['seed'])\n",
        "\n",
        "    # 2. Load Data\n",
        "    model_type = config['model'].split('-')[0]\n",
        "\n",
        "    train_dataloader, eval_dataloaders, num_labels, _ = load_data(\n",
        "        config['task'], config['model']\n",
        "    )\n",
        "\n",
        "    # 3. Load Metric\n",
        "    metric = evaluate.load(\"glue\", config['task'])\n",
        "\n",
        "    # 4. Load Model\n",
        "    # Pass the lora_config from the main config\n",
        "    model = get_model(\n",
        "        config['strategy'],\n",
        "        config['model'],\n",
        "        num_labels,\n",
        "        config['lora_config'],  # <-- Pass lora_config\n",
        "        dynamic_config=config['dynamic_config']\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Setup Optimizer and Scheduler\n",
        "    # Use 'lr' from config\n",
        "    optimizer = create_optimizer(model, config['lr'])\n",
        "    num_training_steps = config['epochs'] * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    # 6. Setup Dynamic Check Frequency\n",
        "    steps_per_epoch = len(train_dataloader)\n",
        "    num_updates=6\n",
        "    if config['dynamic_config'] is not None:\n",
        "      num_updates = config['dynamic_config'].get('num_updates_per_epoch', 6)\n",
        "    print(f\"num updates per epoch: {num_updates}\")\n",
        "    dynamic_check_frequency = max(1, steps_per_epoch // num_updates)\n",
        "    print(f\"Dataset has {steps_per_epoch} steps/epoch. Dynamic check will run every {dynamic_check_frequency} steps.\")\n",
        "\n",
        "\n",
        "    # 7. Training\n",
        "    global_step = 0\n",
        "    all_results = []\n",
        "\n",
        "    gradient_accumulator.clear()\n",
        "    freezing_log.clear()\n",
        "\n",
        "    print(f\"\\n--- Starting Training (ID: {config['exp_id']}) ---\")\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(\n",
        "            train_dataloader,\n",
        "            desc=f\"Epoch {epoch+1}/{config['epochs']}\",\n",
        "            leave=False\n",
        "        )\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # ... (forward pass, loss, backward) ...\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # --- DYNAMIC STRATEGY LOGIC ---\n",
        "            if config['strategy'] == 'dynamic_grad_norm':\n",
        "                accumulate_gradients(model, model_type)\n",
        "\n",
        "                # Use the new dynamic frequency\n",
        "                if (global_step + 1) % dynamic_check_frequency == 0:\n",
        "                    params_changed = update_frozen_layers_HYBRID(\n",
        "                        model,\n",
        "                        model_type,\n",
        "                        # Pass threshold from config\n",
        "                        config['dynamic_config']['threshold_percentile'],\n",
        "                        global_step\n",
        "                    )\n",
        "\n",
        "                    if params_changed:\n",
        "                        print(\"Parameters changed, re-creating optimizer.\")\n",
        "                        optimizer = create_optimizer(model, config['lr']) # Use config lr\n",
        "                        lr_scheduler = get_scheduler(\n",
        "                            \"linear\",\n",
        "                            optimizer=optimizer,\n",
        "                            num_warmup_steps=0,\n",
        "                            num_training_steps=num_training_steps\n",
        "                        )\n",
        "                        lr_scheduler.last_epoch = global_step\n",
        "                        print(f\"Set new scheduler's last_epoch to {global_step}\")\n",
        "\n",
        "            # ... (optimizer step, zero_grad, etc.) ...\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # 8. Evaluation\n",
        "\n",
        "        print(f\"Running evaluation for epoch {epoch+1}...\")\n",
        "        for eval_split_name, eval_dataloader in eval_dataloaders.items():\n",
        "            metric = evaluate.load(\"glue\", config['task'])\n",
        "            model.eval()\n",
        "\n",
        "            for batch in eval_dataloader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**batch)\n",
        "\n",
        "                logits = outputs.logits\n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                metric.add_batch(\n",
        "                    predictions=predictions,\n",
        "                    references=batch[\"labels\"]\n",
        "                )\n",
        "            eval_metric = metric.compute()\n",
        "            print(f\"Epoch {epoch+1} metrics for {eval_split_name}: {eval_metric}\")\n",
        "\n",
        "            run_result = {\n",
        "                # Log all config params for easy filtering\n",
        "                **config,\n",
        "                'epoch': epoch + 1,\n",
        "                'eval_split': eval_split_name,\n",
        "                'metrics': eval_metric,\n",
        "                'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "                'total_params': sum(p.numel() for p in model.parameters()),\n",
        "            }\n",
        "            all_results.append(run_result)\n",
        "\n",
        "    print(f\"--- Finished Training ---\")\n",
        "    return all_results, copy.deepcopy(freezing_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop - Using csv source containing experiment configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Cvps4DG601",
        "outputId": "53d0148c-bd47-47c1-b2b1-010af2830f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 114 previous results into log.\n",
            "Building experiment grid...\n",
            "Loaded 38 experiments from CSV\n",
            "Total experiments loaded      : 38\n",
            "Completed experiments skipped : 38\n",
            "Experiments to run            : 0\n",
            "\n",
            "\n",
            "All experiments complete!\n",
            "Saved final results to: /home/jupyter/NLP_Project_Results_dec7/final_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# --- 6. GRID BUILDER & EXPERIMENT RUNNER (With HASH ID FIX)\n",
        "# ======================================================================\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Load previous intermediate log\n",
        "# ----------------------------------------------------------------------\n",
        "if os.path.exists(intermediate_csv_path):\n",
        "    try:\n",
        "        master_results_log = pd.read_csv(intermediate_csv_path).to_dict('records')\n",
        "        print(f\"Loaded {len(master_results_log)} previous results into log.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        master_results_log = []\n",
        "        print(\"Intermediate file was empty. Starting new log.\")\n",
        "else:\n",
        "    master_results_log = []\n",
        "\n",
        "completed_exp_ids = set(pd.DataFrame(master_results_log)['exp_id'].astype(str)) \\\n",
        "                     if len(master_results_log) > 0 and 'exp_id' in master_results_log[0] else set()\n",
        "\n",
        "print(\"Building experiment grid...\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Deterministic experiment ID\n",
        "# ----------------------------------------------------------------------\n",
        "def generate_exp_id(config_dict):\n",
        "    cfg = dict(config_dict)\n",
        "    cfg.pop(\"exp_id\", None)  # important safeguard\n",
        "    config_str = json.dumps(cfg, sort_keys=True, default=str)\n",
        "    return hashlib.md5(config_str.encode()).hexdigest()[:10]\n",
        "\n",
        "# ======================================================================\n",
        "# üîÅ NEW GRID BUILDER ‚Äî USE CSV INSTEAD OF HARD-CODED SEARCH SPACE\n",
        "# ======================================================================\n",
        "csv_path = \"run_exp_dynamic_grad_norm_25_03_scond.csv\"\n",
        "runs_df = pd.read_csv(csv_path).replace({np.nan: None})\n",
        "\n",
        "total_experiments_in_grid = len(runs_df)\n",
        "print(f\"Loaded {total_experiments_in_grid} experiments from CSV\")\n",
        "\n",
        "def build_config_from_row(row):\n",
        "    \"\"\"Convert a CSV row into a training config dict.\"\"\"\n",
        "    task = row[\"task\"]\n",
        "\n",
        "    # -------- LoRA config --------\n",
        "    lora_cfg = {\n",
        "        \"r\":             row[\"lora_config.r\"],\n",
        "        \"lora_alpha\":    row[\"lora_config.lora_alpha\"],\n",
        "        \"lora_dropout\":  row[\"lora_config.lora_dropout\"],\n",
        "    }\n",
        "\n",
        "    # -------- Dynamic method config --------\n",
        "    if row[\"strategy\"] == \"dynamic_grad_norm\":\n",
        "        dynamic_cfg = {\n",
        "            \"num_updates_per_epoch\":   row[\"dynamic_config.num_updates_per_epoch\"],\n",
        "            \"threshold_percentile\":    row[\"dynamic_config.threshold_percentile\"],\n",
        "        }\n",
        "    else:\n",
        "        dynamic_cfg = None\n",
        "\n",
        "    # -------- Final config --------\n",
        "    return {\n",
        "        \"task\": task,\n",
        "        \"model\": row[\"model\"],\n",
        "        \"strategy\": row[\"strategy\"],\n",
        "        \"seed\": int(row[\"seed\"]),\n",
        "        \"epochs\": EPOCHS_PER_TASK.get(task, 3),\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": float(row[\"lr\"]),\n",
        "        \"lora_config\": lora_cfg,\n",
        "        \"dynamic_config\": dynamic_cfg,\n",
        "    }\n",
        "\n",
        "\n",
        "# Convert CSV rows ‚Üí configs\n",
        "raw_configs = [build_config_from_row(row) for _, row in runs_df.iterrows()]\n",
        "\n",
        "# Add exp_id + filter already completed\n",
        "experiments_to_run = []\n",
        "for cfg in raw_configs:\n",
        "    exp_id = generate_exp_id(cfg)\n",
        "    cfg[\"exp_id\"] = exp_id\n",
        "\n",
        "    if exp_id not in completed_exp_ids:\n",
        "        experiments_to_run.append(cfg)\n",
        "\n",
        "print(f\"Total experiments loaded      : {total_experiments_in_grid}\")\n",
        "print(f\"Completed experiments skipped : {len(completed_exp_ids)}\")\n",
        "print(f\"Experiments to run            : {len(experiments_to_run)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# --- PART B: RUN EXPERIMENTS ---\n",
        "# ======================================================================\n",
        "for i, config in enumerate(experiments_to_run):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"--- STARTING EXPERIMENT {i+1}/{len(experiments_to_run)} (ID: {config['exp_id']}) ---\")\n",
        "    print(f\"Config: {config}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    try:\n",
        "        start = time.time()\n",
        "        run_results, run_grad_log = custom_train(config)\n",
        "        duration = time.time() - start\n",
        "\n",
        "        # annotate results\n",
        "        for r in run_results:\n",
        "            r[\"training_time_seconds\"] = duration\n",
        "\n",
        "        master_results_log.extend(run_results)\n",
        "\n",
        "        # Save dynamic logs\n",
        "        if config[\"strategy\"].startswith(\"dynamic_\"):\n",
        "            log_path = os.path.join(SAVE_PATH, f\"grad_log_{config['exp_id']}_{config['task']}_{config['model']}.json\")\n",
        "            with open(log_path, \"w\") as f:\n",
        "                json.dump(run_grad_log, f, indent=2)\n",
        "\n",
        "        # Save intermediate log\n",
        "        pd.json_normalize(master_results_log).to_csv(intermediate_csv_path, index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{'!'*20} EXPERIMENT FAILED {'!'*20}\")\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "        fail_rec = {\"exp_id\": config[\"exp_id\"], \"error\": str(e), **config}\n",
        "        master_results_log.append(fail_rec)\n",
        "        pd.json_normalize(master_results_log).to_csv(intermediate_csv_path, index=False)\n",
        "\n",
        "    print(f\"--- FINISHED EXPERIMENT {i+1}/{len(experiments_to_run)} ---\")\n",
        "\n",
        "print(\"\\n\\nAll experiments complete!\")\n",
        "\n",
        "final_csv = os.path.join(SAVE_PATH, \"final_experiment_results.csv\")\n",
        "pd.json_normalize(master_results_log).to_csv(final_csv, index=False)\n",
        "print(\"Saved final results to:\", final_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Loop - Building grid from existing configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8oKL6_akm34"
      },
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# --- 6. GRID BUILDER & EXPERIMENT RUNNER (With HASH ID FIX) ---\n",
        "# ======================================================================\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "# --- PART A: THE GRID BUILDER (FIXED) ---\n",
        "\n",
        "# Load previous results so we just append to the log\n",
        "if os.path.exists(intermediate_csv_path):\n",
        "    try:\n",
        "        master_results_log = pd.read_csv(intermediate_csv_path).to_dict('records')\n",
        "        print(f\"Loaded {len(master_results_log)} previous results into log.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        master_results_log = []\n",
        "        print(\"Intermediate file was empty. Starting new log.\")\n",
        "else:\n",
        "    master_results_log = []\n",
        "\n",
        "experiments_to_run = []\n",
        "total_experiments_in_grid = 0\n",
        "\n",
        "print(\"Building experiment grid...\")\n",
        "\n",
        "# This helper function creates the unique, deterministic hash\n",
        "def generate_exp_id(config_dict):\n",
        "    # Create a canonical JSON string (sorted keys ensure order is always the same)\n",
        "    config_str = json.dumps(config_dict, sort_keys=True, default=str)\n",
        "    # Create an MD5 hash and take the first 10 chars for a unique, short ID\n",
        "    return hashlib.md5(config_str.encode()).hexdigest()[:10]\n",
        "\n",
        "for task in GLUE_TASKS:\n",
        "    for model_cp in MODEL_CHECKPOINTS:\n",
        "        for seed in SEEDS:\n",
        "            for strategy in STRATEGIES:\n",
        "\n",
        "                if strategy == 'full_ft':\n",
        "                    for lr in LEARNING_RATES:\n",
        "                        config = {\n",
        "                            'task': task, 'model': model_cp, 'strategy': strategy,\n",
        "                            'seed': seed, 'epochs': EPOCHS_PER_TASK.get(task, 3),\n",
        "                            'batch_size': BATCH_SIZE, 'lr': lr,\n",
        "                            'lora_config': {}, 'dynamic_config': None\n",
        "                        }\n",
        "\n",
        "                        # --- THIS IS THE NEW LOGIC ---\n",
        "                        exp_id_str = generate_exp_id(config)\n",
        "                        config['exp_id'] = exp_id_str\n",
        "                        total_experiments_in_grid += 1\n",
        "                        if exp_id_str not in completed_exp_ids:\n",
        "                          experiments_to_run.append(config)\n",
        "                        # --- END NEW LOGIC ---\n",
        "\n",
        "                elif strategy == 'lora':\n",
        "                    for lr in LEARNING_RATES:\n",
        "                        for lora_cfg in LORA_CONFIGS:\n",
        "                            config = {\n",
        "                                'task': task, 'model': model_cp, 'strategy': strategy,\n",
        "                                'seed': seed, 'epochs': EPOCHS_PER_TASK.get(task, 3),\n",
        "                                'batch_size': BATCH_SIZE, 'lr': lr,\n",
        "                                'lora_config': lora_cfg, 'dynamic_config': None\n",
        "                            }\n",
        "\n",
        "                            # --- THIS IS THE NEW LOGIC ---\n",
        "                            exp_id_str = generate_exp_id(config)\n",
        "                            config['exp_id'] = exp_id_str\n",
        "                            total_experiments_in_grid += 1\n",
        "                            if exp_id_str not in completed_exp_ids:\n",
        "                              experiments_to_run.append(config)\n",
        "                            # --- END NEW LOGIC ---\n",
        "\n",
        "                elif strategy == 'dynamic_grad_norm':\n",
        "                    for lr in LEARNING_RATES:\n",
        "                        for lora_cfg in LORA_CONFIGS:\n",
        "                            for dynamic_cfg in DYNAMIC_CONFIGS:\n",
        "                                config = {\n",
        "                                    'task': task, 'model': model_cp, 'strategy': strategy,\n",
        "                                    'seed': seed, 'epochs': EPOCHS_PER_TASK.get(task, 3),\n",
        "                                    'batch_size': BATCH_SIZE, 'lr': lr,\n",
        "                                    'lora_config': lora_cfg, 'dynamic_config': dynamic_cfg\n",
        "                                }\n",
        "\n",
        "                                # --- THIS IS THE NEW LOGIC ---\n",
        "                                exp_id_str = generate_exp_id(config)\n",
        "                                config['exp_id'] = exp_id_str\n",
        "                                total_experiments_in_grid += 1\n",
        "                                if exp_id_str not in completed_exp_ids:\n",
        "                                    experiments_to_run.append(config)\n",
        "                                # --- END NEW LOGIC ---\n",
        "\n",
        "                elif strategy == 'bitfit':\n",
        "                    for lr in LEARNING_RATES:\n",
        "                        config = {\n",
        "                            'task': task, 'model': model_cp, 'strategy': strategy,\n",
        "                            'seed': seed, 'epochs': EPOCHS_PER_TASK.get(task, 3),\n",
        "                            'batch_size': BATCH_SIZE, 'lr': lr,\n",
        "                            'lora_config': {}, 'dynamic_config': None\n",
        "                        }\n",
        "                        exp_id_str = generate_exp_id(config)\n",
        "                        config['exp_id'] = exp_id_str\n",
        "                        total_experiments_in_grid += 1\n",
        "                        if exp_id_str not in completed_exp_ids:\n",
        "                            experiments_to_run.append(config)\n",
        "\n",
        "                elif strategy == 'topk_full':\n",
        "                    for lr in LEARNING_RATES:\n",
        "                      for k in TOPK_VALUES:\n",
        "                          config = {\n",
        "                              'task': task, 'model': model_cp, 'strategy': strategy,\n",
        "                              'seed': seed, 'epochs': EPOCHS_PER_TASK.get(task, 3),\n",
        "                              'batch_size': BATCH_SIZE, 'lr': lr,\n",
        "                              'lora_config': {}, 'dynamic_config': {'topk_layers' : k}\n",
        "                              # If you later want different k values, you can add 'topk_layers': k here.\n",
        "                          }\n",
        "                          exp_id_str = generate_exp_id(config)\n",
        "                          config['exp_id'] = exp_id_str\n",
        "                          total_experiments_in_grid += 1\n",
        "                          if exp_id_str not in completed_exp_ids:\n",
        "                              experiments_to_run.append(config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"--- Total experiments in grid: {total_experiments_in_grid} ---\")\n",
        "print(f\"--- Completed: {len(completed_exp_ids)}. New experiments to run: {len(experiments_to_run)} ---\")\n",
        "\n",
        "\n",
        "# --- PART B: THE EXPERIMENT RUNNER ---\n",
        "# We now iterate over 'experiments_to_run'\n",
        "for i, config in enumerate(experiments_to_run):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    # This now shows progress on the *remaining* jobs\n",
        "    print(f\"--- STARTING EXPERIMENT {i+1}/{len(experiments_to_run)} (ID: {config['exp_id']}) ---\")\n",
        "    print(f\"Config: {config}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        run_results, run_grad_log = custom_train(config)\n",
        "        end_time = time.time()\n",
        "        duration_seconds = end_time - start_time\n",
        "        print(f\"Total training time for this run: {duration_seconds:.2f} seconds\")\n",
        "\n",
        "        for res in run_results:\n",
        "            res['training_time_seconds'] = duration_seconds\n",
        "\n",
        "        master_results_log.extend(run_results)\n",
        "\n",
        "        exp_id = config['exp_id']\n",
        "\n",
        "        if config['strategy'].startswith('dynamic_'):\n",
        "            log_filename = f\"grad_log_{exp_id}_{config['task']}_{config['model']}.json\"\n",
        "            json_save_path = os.path.join(SAVE_PATH, log_filename)\n",
        "            print(f\"Saving gradient log to {json_save_path}\")\n",
        "            with open(json_save_path, 'w') as f:\n",
        "                json.dump(run_grad_log, f, indent=2)\n",
        "\n",
        "        # Save intermediate CSV\n",
        "        csv_save_path = os.path.join(SAVE_PATH, \"master_results_log_intermediate.csv\")\n",
        "        # Save the *entire* log (old + new)\n",
        "        pd.json_normalize(master_results_log).to_csv(csv_save_path, index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{'!'*20} EXPERIMENT FAILED {'!'*20}\")\n",
        "        print(f\"Failed on config: {config}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        master_results_log.append({ 'exp_id': config['exp_id'], 'error': str(e), **config })\n",
        "        # Save the log even on failure\n",
        "        csv_save_path = os.path.join(SAVE_PATH, \"master_results_log_intermediate.csv\")\n",
        "        pd.json_normalize(master_results_log).to_csv(csv_save_path, index=False)\n",
        "\n",
        "    print(f\"--- FINISHED EXPERIMENT {i+1}/{len(experiments_to_run)} ---\")\n",
        "\n",
        "print(\"\\n\\nAll experiments complete!\")\n",
        "\n",
        "# --- FINAL SAVE ---\n",
        "final_csv_path = os.path.join(SAVE_PATH, \"final_experiment_results.csv\")\n",
        "pd.json_normalize(master_results_log).to_csv(final_csv_path, index=False)\n",
        "print(f\"Saved final results to {final_csv_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "name": "freezelora3_(8)_(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
