{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoMode GSM8K Fine-Tuning\n",
        "This notebook implements a dynamic fine-tuning strategy for GSM8K using Gemma-2B.\n",
        "It supports:\n",
        "- Full Fine-Tuning\n",
        "- LoRA\n",
        "- Dynamic Gradient Norm (Hybrid)\n",
        "- BitFit\n",
        "- Top-K Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "Install necessary libraries including `transformers`, `peft`, `bitsandbytes`, and `accelerate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q \"transformers>=4.45.0\" \"datasets\" \"accelerate\" \"einops\" \"sentencepiece\" \"protobuf==3.20.3\"\n",
        "!pip install -q \"huggingface_hub[cli]\" \"peft\" \"bitsandbytes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration & Imports\n",
        "Define Global Configuration and Experiment Settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import math, torch, json, os, time\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler\n",
        "from bitsandbytes.optim import AdamW8bit\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from torch.utils.data import DataLoader\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "SAVE_PATH = \"/home/jupyter/GSM8K_Results_dec9\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "DATA_CACHE_PATH = f\"{SAVE_PATH}/gsm8k_tokenized.arrow\"\n",
        "\n",
        "GSM8K_CONFIG = {\n",
        "    \"model_checkpoint\": \"google/gemma-2-2b\",\n",
        "    \"seed\": 42,\n",
        "    \"max_input_length\": 2048,\n",
        "\n",
        "    # Training\n",
        "    \"batch_train\": 1,\n",
        "    \"batch_eval\": 8,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"epochs\": 3,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"grad_accum\": 16,\n",
        "\n",
        "    # Strategy options\n",
        "    \"strategy\": \"dynamic_grad_norm\",  # \"full_ft\", \"lora\", \"dynamic_grad_norm\"\n",
        "\n",
        "    # Dynamic FT\n",
        "    \"dynamic_updates\": 6,\n",
        "    \"dynamic_threshold\": 10,\n",
        "\n",
        "    # Misc\n",
        "    \"fp16\": True,\n",
        "\n",
        "    # Generation for evaluation\n",
        "    \"gen_max_tokens\": 128,\n",
        "    \"sampling_k\": 5,\n",
        "}\n",
        "\n",
        "gradient_accumulator = defaultdict(lambda: (0.0, 0))\n",
        "freezing_log = []\n",
        "model_name = GSM8K_CONFIG[\"model_checkpoint\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Experiment Grid\n",
        "Define the set of experiments to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import pandas as pd\n",
        "\n",
        "RESULTS_CSV = f\"{SAVE_PATH}/experiments_log.csv\"\n",
        "\n",
        "EXPERIMENT_GRID = [\n",
        "    {\"strategy\": \"dynamic_grad_norm\", \"dynamic_updates\": 6, \"dynamic_threshold\": 10, \"sampling_k\": 5, \"epochs\": 2},\n",
        "    {\"strategy\": \"dynamic_grad_norm\", \"dynamic_updates\": 6, \"dynamic_threshold\": 25, \"sampling_k\": 5, \"epochs\": 2},\n",
        "    {\"strategy\": \"dynamic_grad_norm\", \"dynamic_updates\": 10, \"dynamic_threshold\": 10, \"sampling_k\": 5, \"epochs\": 2},\n",
        "    {\"strategy\": \"dynamic_grad_norm\", \"dynamic_updates\": 10, \"dynamic_threshold\": 25, \"sampling_k\": 5, \"epochs\": 2},\n",
        "    {\"strategy\": \"lora\",               \"sampling_k\": 5, \"epochs\": 2},\n",
        "    {\"strategy\": \"full_ft\",            \"sampling_k\": 5, \"epochs\": 2},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Reproducibility\n",
        "Set random seeds for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(GSM8K_CONFIG[\"seed\"])\n",
        "random.seed(GSM8K_CONFIG[\"seed\"])\n",
        "np.random.seed(GSM8K_CONFIG[\"seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Utility Functions\n",
        "Helper functions for answer extraction, prompt building, logging, and result persistence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Answer Extractor ----------\n",
        "def extract_final_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the final numerical answer from the model output.\n",
        "    Looks for '#### <number>' pattern or takes the last number found.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"####\\s*([-\\d,\\.]+)\", text)\n",
        "    if m:\n",
        "        return m.group(1).replace(\",\", \"\").strip()\n",
        "    nums = re.findall(r\"[-]?\\d[\\d,\\.]*\", text)\n",
        "    return nums[-1].replace(\",\", \"\") if nums else \"\"\n",
        "\n",
        "# ---------- Instruction ----------\n",
        "INSTRUCTION = (\n",
        "    \"You are an expert grade-school math tutor. \"\n",
        "    \"Solve the problem step by step, then give the final numeric answer on \"\n",
        "    \"a separate line as:\\n#### <NUMERIC_ANSWER>\\n\\nQuestion:\\n\"\n",
        ")\n",
        "\n",
        "def build_prompt(q: str) -> str:\n",
        "    return INSTRUCTION + q + \"\\n\\nAnswer:\\n\"\n",
        "\n",
        "# ---------- Logging ----------\n",
        "def timestamp():\n",
        "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "    print(f\"üíæ Saved: {path}\")\n",
        "\n",
        "def make_exp_id(config: dict):\n",
        "    cfg = dict(sorted(config.items()))\n",
        "    s = json.dumps(cfg, sort_keys=True)\n",
        "    return hashlib.md5(s.encode()).hexdigest()[:10]\n",
        "\n",
        "def load_results():\n",
        "    if os.path.exists(RESULTS_CSV):\n",
        "        return pd.read_csv(RESULTS_CSV)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def append_result(row: dict):\n",
        "    df = load_results()\n",
        "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    df.to_csv(RESULTS_CSV, index=False)\n",
        "    print(f\"üìå Logged exp_id={row['exp_id']} to CSV: {RESULTS_CSV}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Authentication\n",
        "Login to Hugging Face Hub (required for accessing Gemma models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_TOKEN = \"\"  # Add your Hugging Face token here\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model & Fine-Tuning Strategies\n",
        "Core logic for:\n",
        "- Gradient Accumulation (for dynamic strategies)\n",
        "- Model Initialization (Full FT, LoRA, etc.)\n",
        "- Dynamic Layer Freezing/Unfreezing\n",
        "- BitFit and Top-K implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_layer_name_from_param(param_name: str) -> str:\n",
        "    \"\"\"\n",
        "    For GemmaForCausalLM (and LoRA-wrapped versions), param names look like:\n",
        "    'model.layers.0.self_attn.q_proj.lora_A.default.weight'\n",
        "    We group by 'model.layers.N'.\n",
        "    \"\"\"\n",
        "    parts = param_name.split(\".\")\n",
        "    if \"layers\" in parts:\n",
        "        idx = parts.index(\"layers\")\n",
        "        if idx + 1 < len(parts):\n",
        "            return \".\".join(parts[:idx+2])  # 'model.layers.0'\n",
        "    return \"other_params\"\n",
        "\n",
        "def accumulate_gradients(model):\n",
        "    \"\"\"\n",
        "    Accumulate squared gradient norms per logical layer.\n",
        "    \"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None and param.requires_grad:\n",
        "            layer_name = get_layer_name_from_param(name)\n",
        "            if layer_name != \"other_params\":\n",
        "                current_norm_sq, current_count = gradient_accumulator[layer_name]\n",
        "                new_norm_sq = current_norm_sq + (torch.norm(param.grad, p=2).item() ** 2)\n",
        "                new_count = current_count + param.numel()\n",
        "                gradient_accumulator[layer_name] = (new_norm_sq, new_count)\n",
        "\n",
        "def get_layer_name_from_module(module_name: str) -> str:\n",
        "    return get_layer_name_from_param(module_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(strategy: str) -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    strategy: 'full_ft', 'lora', 'dynamic_grad_norm'\n",
        "    \"\"\"\n",
        "    print(f\"Loading Gemma-2B with strategy = {strategy}\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if GSM8K_CONFIG[\"fp16\"] and torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"cuda\",\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    # gradient checkpointing to save VRAM\n",
        "    base_model.gradient_checkpointing_enable()\n",
        "    base_model.enable_input_require_grads()\n",
        "\n",
        "    if strategy in [\"full_ft\",'bitfit','topk_full']:\n",
        "        return base_model\n",
        "\n",
        "    # LoRA configs (q_proj + v_proj only)\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base_model, lora_cfg)\n",
        "    if hasattr(model, \"lm_head\"):\n",
        "      for p in model.lm_head.parameters():\n",
        "          p.requires_grad = True\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # For 'lora' we keep base frozen, LoRA trainable.\n",
        "    # For 'dynamic_grad_norm' we start in LoRA-only mode,\n",
        "    # then selectively merge/unmerge via update_frozen_layers_HYBRID.\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_frozen_layers_HYBRID(model, threshold_percentile: float, global_step: int):\n",
        "    \"\"\"\n",
        "    Dynamic Hybrid Fine-Tuning for Gemma CausalLM:\n",
        "      - Computes average grad norm per layer\n",
        "      - Layers above threshold -> Full-FT\n",
        "      - Layers below threshold -> LoRA-Frozen\n",
        "\n",
        "    Logs:\n",
        "      - step\n",
        "      - threshold\n",
        "      - per-layer norm + action\n",
        "      - current trainable params\n",
        "    \"\"\"\n",
        "    if not gradient_accumulator:\n",
        "        return False\n",
        "\n",
        "    # 1. average gradient norms\n",
        "    avg_layer_norms = {}\n",
        "    for layer, (sum_sq_norm, param_count) in gradient_accumulator.items():\n",
        "        if param_count > 0:\n",
        "            avg_norm = (sum_sq_norm / (param_count + 1e-9)) ** 0.5\n",
        "            avg_layer_norms[layer] = avg_norm\n",
        "\n",
        "    if not avg_layer_norms:\n",
        "        return False\n",
        "\n",
        "    norms = list(avg_layer_norms.values())\n",
        "    threshold_val = np.percentile(norms, threshold_percentile)\n",
        "\n",
        "    print(f\"\\n--- Dynamic Hybrid Check @ Step {global_step+1} ---\")\n",
        "    print(f\"Average grad-norm {threshold_percentile}th percentile: {threshold_val}\")\n",
        "\n",
        "    target_state_map = {}\n",
        "    log_entry = {\"step\": global_step + 1, \"threshold\": threshold_val, \"layers\": {}}\n",
        "\n",
        "    for layer_name, avg_norm in avg_layer_norms.items():\n",
        "        is_full_ft_target = (avg_norm >= threshold_val)\n",
        "        target_state = \"full_ft\" if is_full_ft_target else \"lora_frozen\"\n",
        "        target_state_map[layer_name] = target_state\n",
        "        log_entry[\"layers\"][layer_name] = {\"norm\": avg_norm, \"action\": target_state}\n",
        "\n",
        "    params_changed = False\n",
        "\n",
        "    # 2. apply changes over LoRA modules\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, LoraLayer):\n",
        "            layer_name = get_layer_name_from_module(module_name)\n",
        "            if layer_name in target_state_map:\n",
        "                target_state = target_state_map[layer_name]\n",
        "                # base layer trainability indicates full-ft or not\n",
        "                is_currently_full_ft = next(module.get_base_layer().parameters()).requires_grad\n",
        "\n",
        "                if target_state == \"full_ft\" and not is_currently_full_ft:\n",
        "                    print(f\"Switch {layer_name} -> Full-FT (merge LoRA)\")\n",
        "\n",
        "                    # merge LoRA to base\n",
        "                    module.merge()\n",
        "\n",
        "                    # freeze LoRA params\n",
        "                    for p in module.lora_A.parameters():\n",
        "                        p.requires_grad = False\n",
        "                    for p in module.lora_B.parameters():\n",
        "                        p.requires_grad = False\n",
        "\n",
        "                    # unfreeze base\n",
        "                    for p in module.get_base_layer().parameters():\n",
        "                        p.requires_grad = True\n",
        "\n",
        "                    params_changed = True\n",
        "\n",
        "                elif target_state == \"lora_frozen\" and is_currently_full_ft:\n",
        "                    print(f\"Switch {layer_name} -> LoRA-Frozen (reset adapters)\")\n",
        "\n",
        "                    # freeze base\n",
        "                    for p in module.get_base_layer().parameters():\n",
        "                        p.requires_grad = False\n",
        "\n",
        "                    # unfreeze LoRA\n",
        "                    for p in module.lora_A.parameters():\n",
        "                        p.requires_grad = True\n",
        "                    for p in module.lora_B.parameters():\n",
        "                        p.requires_grad = True\n",
        "\n",
        "                    # reset LoRA parameters\n",
        "                    module.reset_lora_parameters(\"default\", True)\n",
        "\n",
        "                    params_changed = True\n",
        "\n",
        "    if params_changed:\n",
        "        current_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        log_entry[\"current_trainable_params\"] = current_trainable\n",
        "        print(f\"Current trainable params: {current_trainable:,}\")\n",
        "\n",
        "    gradient_accumulator.clear()\n",
        "    freezing_log.append(log_entry)\n",
        "\n",
        "    return params_changed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ GSM8K ‚Äî BitFit and Top-k Full Fine-Tune Utils ============\n",
        "\n",
        "def apply_bitfit_gsm(model):\n",
        "    \"\"\"\n",
        "    BitFit for Causal-LM GSM8K: train only bias terms + LM head.\n",
        "    \"\"\"\n",
        "    # 1. Freeze all parameters\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 2. Unfreeze bias terms\n",
        "    for name, p in model.named_parameters():\n",
        "        last = name.lower().split(\".\")[-1]\n",
        "        if last == \"bias\":\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # 3. Ensure the LM head is trainable (decoder head)\n",
        "    if hasattr(model, \"lm_head\"):\n",
        "        for p in model.lm_head.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    print(\"üü¶ Applied BitFit for GSM8K: train only bias + lm_head.\")\n",
        "\n",
        "\n",
        "def apply_topk_full_ft_gsm(model, k):\n",
        "    \"\"\"\n",
        "    Top-k transformer blocks are fully trainable, rest frozen, + LM head.\n",
        "    Works for LLaMA/Gemma families (AutoModelForCausalLM).\n",
        "    \"\"\"\n",
        "    # 1. Freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 2. Locate transformer blocks\n",
        "    if hasattr(model, \"model\") and hasattr(model.model, \"layers\"):\n",
        "        layers = model.model.layers\n",
        "    elif hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):\n",
        "        layers = model.transformer.h  # GPT-style\n",
        "    else:\n",
        "        raise ValueError(\"‚ùå Could not locate transformer layers for Top-k FT\")\n",
        "\n",
        "    # 3. Select top-k blocks\n",
        "    n = len(layers)\n",
        "    k = min(k, n)\n",
        "    selected = list(range(n - k, n))\n",
        "\n",
        "    print(f\"üü• Top-k Full FT: Unfreezing transformer blocks {selected}\")\n",
        "\n",
        "    for i in selected:\n",
        "        for p in layers[i].parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # 4. Always unfreeze LM head\n",
        "    if hasattr(model, \"lm_head\"):\n",
        "        for p in model.lm_head.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    print(\"üü• Applied Top-k Full FT for GSM8K.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Preparation\n",
        "- `DataCollatorForCausalLM`: Handles padding and label masking.\n",
        "- `load_gsm8k`: Loads and tokenizes the dataset, with caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Data Preparation ----------\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # üßπ Remove text fields before batching\n",
        "        text_fields = [\"question\", \"full_answer_text\", \"answer_str\", \"answer\"]\n",
        "        features_for_model = [\n",
        "            {k: v for k, v in f.items() if k not in text_fields}\n",
        "            for f in features\n",
        "        ]\n",
        "\n",
        "        # Pad input + attention mask\n",
        "        batch = self.tokenizer.pad(\n",
        "            {k: [f[k] for f in features_for_model] if k != \"labels\" else None\n",
        "             for k in features_for_model[0] if k != \"labels\"},\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Pad labels separately, and apply ignore index\n",
        "        labels = self.tokenizer.pad(\n",
        "            {\"input_ids\": [f[\"labels\"] for f in features_for_model]},\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "\n",
        "from datasets import Dataset, load_from_disk\n",
        "\n",
        "def load_gsm8k(tokenizer):\n",
        "    \"\"\"\n",
        "    Loads GSM8K with tokenization cache on disk.\n",
        "    Returns: tokenized dataset (train/test)\n",
        "    \"\"\"\n",
        "    # Check cache\n",
        "    if os.path.exists(DATA_CACHE_PATH):\n",
        "        print(f\"‚ö° Loading cached tokenized dataset from {DATA_CACHE_PATH}\")\n",
        "        return load_from_disk(DATA_CACHE_PATH)\n",
        "\n",
        "    print(\"‚è≥ Tokenizing GSM8K dataset for the first time‚Ä¶ (will be cached)\")\n",
        "\n",
        "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "\n",
        "    def proc(batch):\n",
        "        texts, answers = [], []\n",
        "        for q,a in zip(batch[\"question\"], batch[\"answer\"]):\n",
        "            texts.append(build_prompt(q) + a)\n",
        "            answers.append(extract_final_answer(a))\n",
        "\n",
        "        enc = tokenizer(\n",
        "            texts,\n",
        "            max_length=GSM8K_CONFIG[\"max_input_length\"],\n",
        "            truncation=True,\n",
        "        )\n",
        "        enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
        "        enc[\"answer_str\"] = answers\n",
        "\n",
        "        # Keep raw text (so evaluation can access it)\n",
        "        enc[\"question\"] = batch[\"question\"]\n",
        "        enc[\"full_answer_text\"] = batch[\"answer\"]\n",
        "        return enc\n",
        "\n",
        "    tokenized = dataset.map(proc, batched=True)\n",
        "\n",
        "    print(f\"üíæ Saving tokenized dataset to cache: {DATA_CACHE_PATH}\")\n",
        "    tokenized.save_to_disk(DATA_CACHE_PATH)\n",
        "\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop\n",
        "The main training function `train_model` which:\n",
        "1. Loads the model and dataset.\n",
        "2. Applies the selected strategy.\n",
        "3. Runs the training loop with gradient accumulation.\n",
        "4. Triggers dynamic updates if enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Train Function ----------\n",
        "def train_model(exp_id):\n",
        "    start = time.time()\n",
        "    tok = AutoTokenizer.from_pretrained(GSM8K_CONFIG[\"model_checkpoint\"])\n",
        "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "    ds = load_gsm8k(tok)\n",
        "    raw_ds = load_dataset(\"gsm8k\", \"main\")\n",
        "    sample_raw = raw_ds[\"train\"][0]\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        ds[\"train\"],\n",
        "        batch_size=GSM8K_CONFIG[\"batch_train\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=DataCollatorForCausalLM(tok),\n",
        "    )\n",
        "\n",
        "    # LOAD MODEL (shortened)\n",
        "    strategy = GSM8K_CONFIG[\"strategy\"]\n",
        "    model = get_model(strategy)\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"üîß Trainable Params: {trainable:,} / {total:,}  ({trainable/total*100:.2f}%)\\n\")\n",
        "\n",
        "    print(\"üîß Example Trainable Parameter Names:\")\n",
        "    for name, p in list(model.named_parameters())[:5]:\n",
        "        if p.requires_grad:\n",
        "            print(\"  üè∑Ô∏è\", name)\n",
        "\n",
        "    if strategy == \"bitfit\":\n",
        "        apply_bitfit_gsm(model)\n",
        "\n",
        "    elif strategy == \"topk_full\":\n",
        "        apply_topk_full_ft_gsm(model, k=4)  # or number you choose\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    steps = math.ceil(len(train_dl) * GSM8K_CONFIG[\"epochs\"] / GSM8K_CONFIG[\"grad_accum\"])\n",
        "    optim = AdamW8bit([p for p in model.parameters() if p.requires_grad],\n",
        "                      lr=GSM8K_CONFIG[\"learning_rate\"])\n",
        "    sched = get_scheduler(\n",
        "            name=\"cosine\",\n",
        "            optimizer=optim,\n",
        "            num_warmup_steps=int(GSM8K_CONFIG[\"warmup_ratio\"] * steps),\n",
        "            num_training_steps=steps,\n",
        "        )\n",
        "\n",
        "    progress = tqdm(range(steps))\n",
        "    g = 0\n",
        "    opt_step = 0\n",
        "    for epoch in range(GSM8K_CONFIG[\"epochs\"]):\n",
        "      in_epoch_steps = 0\n",
        "      print(f\"\\n=== üèãÔ∏è Epoch {epoch+1}/{GSM8K_CONFIG['epochs']} ===\")\n",
        "      for batch in train_dl:\n",
        "          batch = {k: v.cuda() for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
        "          loss = model(**batch).loss / GSM8K_CONFIG[\"grad_accum\"]\n",
        "          loss.backward()\n",
        "\n",
        "          # Only accumulate dynamic grads AFTER backward\n",
        "          if strategy == \"dynamic_grad_norm\":\n",
        "              accumulate_gradients(model)\n",
        "\n",
        "          g += 1\n",
        "\n",
        "\n",
        "          # Only trigger on optimizer steps\n",
        "          if g % GSM8K_CONFIG[\"grad_accum\"] == 0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optim.step()\n",
        "              sched.step()\n",
        "              optim.zero_grad()\n",
        "\n",
        "              opt_step += 1  # <-- count real steps\n",
        "              in_epoch_steps += 1\n",
        "\n",
        "              progress.update(1)\n",
        "              progress.set_description(f\"loss={loss.item():.4f}\")\n",
        "\n",
        "              # Dynamic Logic ON REAL STEPS ONLY\n",
        "              if strategy == \"dynamic_grad_norm\":\n",
        "                  interval = max(1, (steps/GSM8K_CONFIG[\"epochs\"]) // GSM8K_CONFIG[\"dynamic_updates\"])\n",
        "                  if (in_epoch_steps % interval == 0) or (in_epoch_steps == steps/GSM8K_CONFIG[\"epochs\"]):\n",
        "                      changed = update_frozen_layers_HYBRID(\n",
        "                          model, GSM8K_CONFIG[\"dynamic_threshold\"], opt_step\n",
        "                      )\n",
        "                      if changed:\n",
        "                          optim = AdamW8bit([p for p in model.parameters() if p.requires_grad],\n",
        "                                            lr=GSM8K_CONFIG[\"learning_rate\"])\n",
        "                          sched = get_scheduler(\n",
        "                              name=\"cosine\", optimizer=optim,\n",
        "                              num_warmup_steps=max(1, int(GSM8K_CONFIG[\"warmup_ratio\"] * steps)),\n",
        "                              num_training_steps=steps,\n",
        "                          )\n",
        "    runtime = time.time()-start\n",
        "    save_json(freezing_log, f\"{SAVE_PATH}/dynamic_log_{exp_id}.json\")\n",
        "    print(f\"‚è±Ô∏è Training time: {runtime:.2f}s\")\n",
        "    return model, tok,ds, runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation\n",
        "`evaluate`: Generates answers for the test set and calculates majority voting accuracy (maj@1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, tok, ds_test):\n",
        "    model.eval()\n",
        "    B = GSM8K_CONFIG[\"batch_eval\"]\n",
        "    k = GSM8K_CONFIG[\"sampling_k\"]\n",
        "    max_t = GSM8K_CONFIG[\"gen_max_tokens\"]\n",
        "\n",
        "    logs = []\n",
        "    correct = 0\n",
        "    N = len(ds_test)\n",
        "    for i in tqdm(range(0, N, B), desc=\"Batch maj@1\"):\n",
        "        batch = ds_test[i:i+B]\n",
        "        prompts = [build_prompt(q) for q in batch[\"question\"]]\n",
        "        inp = tok(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(\n",
        "                **inp, max_new_tokens=max_t, do_sample=True,\n",
        "                temperature=0.7, top_p=0.9,\n",
        "                num_return_sequences=k,\n",
        "                pad_token_id=tok.pad_token_id,\n",
        "                eos_token_id=tok.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # unpack generations\n",
        "        seqs = out.reshape(len(prompts), k, -1)\n",
        "        for j,prompt in enumerate(prompts):\n",
        "            gold = extract_final_answer(batch[\"answer\"][j])\n",
        "            texts = [tok.decode(seqs[j][l][inp[\"input_ids\"].shape[1]:],\n",
        "                                skip_special_tokens=True)\n",
        "                     for l in range(k)]\n",
        "            preds = [extract_final_answer(t) for t in texts]\n",
        "            maj = max(set(preds), key=preds.count)\n",
        "            correct += (maj==gold)\n",
        "            logs.append({\"question\": batch[\"question\"][j],\n",
        "                         \"gold\": gold, \"samples\": preds, \"maj_pred\": maj})\n",
        "            if i%50==0:\n",
        "              print(\"\\nüìå Extracted Predictions:\", preds)\n",
        "              print(\"üèÜ Majority Prediction:\", maj)\n",
        "              print(\"Gold:\", gold)\n",
        "              print(\"====================================\\n\")\n",
        "    acc = correct/N\n",
        "    path = f\"{SAVE_PATH}/full_eval_{acc:.3f}.json\"\n",
        "    save_json(logs, path)\n",
        "\n",
        "    print(f\"\\nmaj@1={acc*100:.2f}%\")\n",
        "    return acc, path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Experiment Runner\n",
        "Orchestrates the experiments defined in the grid, running training and evaluation for each configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import json\n",
        "def run_experiment(config):\n",
        "    global GSM8K_CONFIG, gradient_accumulator, freezing_log\n",
        "\n",
        "    # Reset dynamic global state\n",
        "    gradient_accumulator = defaultdict(lambda: (0.0, 0))\n",
        "    freezing_log = []\n",
        "\n",
        "    # Apply config values to main config\n",
        "    for k, v in config.items():\n",
        "        GSM8K_CONFIG[k] = v\n",
        "\n",
        "    exp_id = make_exp_id(config)\n",
        "    print(f\"\\nüöÄ Running experiment {exp_id} with config={config}\")\n",
        "\n",
        "    # === TRAIN ===\n",
        "    model, tok, ds, runtime = train_model(exp_id)\n",
        "\n",
        "    # === EVAL (subset. change to full test when stable) ===\n",
        "    acc, _ = evaluate(model, tok, ds[\"test\"].select(range(400)))\n",
        "\n",
        "    # === COUNT PARAMS ===\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # === LOG ROW ===\n",
        "    result = {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"strategy\": GSM8K_CONFIG[\"strategy\"],\n",
        "        \"dynamic_updates\": GSM8K_CONFIG.get(\"dynamic_updates\", None),\n",
        "        \"learning_rate\": GSM8K_CONFIG[\"learning_rate\"],\n",
        "        \"dynamic_threshold\": GSM8K_CONFIG.get(\"dynamic_threshold\", None),\n",
        "        \"sampling_k\": GSM8K_CONFIG.get(\"sampling_k\", None),\n",
        "        \"epochs\": GSM8K_CONFIG.get(\"epochs\", None),\n",
        "        \"trainable_params\": trainable,\n",
        "        \"total_params\": total,\n",
        "        \"trainable_pct\": trainable / total,\n",
        "        \"maj@1\": acc,\n",
        "        \"runtime_sec\": runtime,\n",
        "        \"timestamp\": timestamp()\n",
        "    }\n",
        "    append_result(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Main Execution\n",
        "Iterate through the `EXPERIMENT_GRID` and run all experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for cfg in EXPERIMENT_GRID:\n",
        "    row = run_experiment(cfg)\n",
        "    print(\"‚úîÔ∏è Finished:\", row)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
